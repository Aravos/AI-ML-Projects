{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58c4f77-e906-471a-9a73-52a06c04fd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "import io\n",
    "import numpy as np\n",
    "from IPython.display import clear_output, Image, display\n",
    "import PIL.Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933305b5-02db-4609-bf63-61a253622a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showarray(a, fmt='jpeg'):\n",
    "    if isinstance(a, list):\n",
    "        imgs = [np.uint8(np.clip(img, 0, 255)) for img in a]\n",
    "        f = io.BytesIO()\n",
    "        PIL.Image.fromarray(np.concatenate(imgs, axis=1)).save(f, fmt)\n",
    "        display(Image(data=f.getvalue()))\n",
    "    else:\n",
    "        a = np.uint8(np.clip(a, 0, 255))\n",
    "        f = io.BytesIO()\n",
    "        PIL.Image.fromarray(a).save(f, fmt)\n",
    "        display(Image(data=f.getvalue()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bda0232-e55a-4943-8260-bca98210f162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (100, 100))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "def old_preprocess_image(img_path): # Doesn't Work that well\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (100, 100))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)  # HSV = hue, saturation and value\n",
    "    lower_skin = np.array([0, 20, 70])\n",
    "    upper_skin = np.array([25, 255, 255])\n",
    "    # Mask for skin color detection\n",
    "    mask_skin = cv2.inRange(img_hsv, lower_skin, upper_skin)\n",
    "    img_blur = cv2.GaussianBlur(img, (3, 3), 0)\n",
    "    img_gray = cv2.cvtColor(img_blur, cv2.COLOR_RGB2GRAY)\n",
    "    # Sobel Edge Detection \n",
    "    sobelx = cv2.Sobel(img_gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    sobely = cv2.Sobel(img_gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    laplacian = cv2.Laplacian(img_gray, cv2.CV_64F) \n",
    "    sobel = np.sqrt(sobelx**2 + sobely**2)\n",
    "    # Apply skin mask\n",
    "    img = cv2.bitwise_and(sobel, sobel, mask=mask_skin)\n",
    "    img = np.uint8(np.clip(img, 0, 255))\n",
    "    # Apply threshold\n",
    "    _, img = cv2.threshold(img, 25, 50, cv2.THRESH_BINARY)\n",
    "    # Convert single channel to 3 channels (RGB)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "    return img_rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ede0bc-3b27-48a6-b497-39c935232efd",
   "metadata": {},
   "source": [
    "DataLoader Implementation (87000 images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2504b0cc-4640-4f6d-ac7a-ee74d3550bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datatypes = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "data_path = '/home/aravos/ASL-data/asl_alphabet_train/asl_alphabet_train'  # Use Unix-style paths\n",
    "data = {i: [] for i in datatypes}\n",
    "mp = {datatypes[i]: i for i in range(len(datatypes))}\n",
    "for dtype in data.keys():\n",
    "    temp_path = os.path.join(data_path, dtype)\n",
    "    for filename in os.listdir(temp_path):\n",
    "        if filename.endswith(('.png', '.jpg', '.jpeg', '.bmp')) and 'Zone.Identifier' not in filename:\n",
    "            data[dtype].append(os.path.join(temp_path, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b36cef9-5beb-4910-8641-6c6ff29d1209",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AslDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x = []\n",
    "        self.labels = []\n",
    "        #self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        for dtype in data.keys():\n",
    "            for i in data[dtype]:\n",
    "                self.labels.append(mp[dtype])\n",
    "                img_tensor = self.transform(preprocess_image(i))\n",
    "                self.x.append(img_tensor)\n",
    "        \n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.long)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index].cpu(), self.labels[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f6a781-aba7-47e2-ac7c-ed1622491540",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AslDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccffafa-3bdb-45e4-834b-6d7965100f6c",
   "metadata": {},
   "source": [
    "Creating dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2786e9-a83d-42c9-a8ec-12388f54f2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462e8a4e-a766-4ace-8d37-4edbbb71848f",
   "metadata": {},
   "source": [
    "CNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154db68d-df56-492b-9b1f-38ae9f101c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 40\n",
    "batch_size = 128\n",
    "train_test_split = 0.8\n",
    "num_classes = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e4ecca-70a8-4da6-a5e3-530cb2ad0c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space')\n",
    "print(len(classes))\n",
    "def imshow(img):\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "l = []\n",
    "for i in range (len(images)):\n",
    "    img = images[i].numpy()\n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    img = np.uint8(img * 255)\n",
    "    l.append(img)\n",
    "# show images\n",
    "showarray(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6b8c28-ba1c-4fe9-af0f-de46a1167090",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)   # Output: 6 x 96 x 96\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)  # Output: 16 x 92 x 92\n",
    "        self.conv3 = nn.Conv2d(16, 40, 5) # Output: 40 x 88 x 88\n",
    "        self.conv4 = nn.Conv2d(40, 100, 5)# Output: 100 x 84 x 84\n",
    "        self.fc1 = nn.Linear(100 * 19 * 19, 1000) # Flatten: 100 x 19 x 19 = 36100\n",
    "        self.fc2 = nn.Linear(1000, 500)\n",
    "        self.fc3 = nn.Linear(500, 29)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))           # Output: 6 x 96 x 96\n",
    "        x = self.pool(F.relu(self.conv2(x)))# Output: 16 x 46 x 46\n",
    "        x = F.relu(self.conv3(x))           # Output: 40 x 42 x 42\n",
    "        x = self.pool(F.relu(self.conv4(x)))# Output: 100 x 19 x 19\n",
    "        x = x.view(-1, 100 * 19 * 19)       # Flatten: 100 x 19 x 19 = 36100\n",
    "        x = F.relu(self.fc1(x))             # Output: 1000\n",
    "        x = F.relu(self.fc2(x))             # Output: 500\n",
    "        x = self.fc3(x)                     # Output: 29 (num_classes)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = ConvNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4484809d-9cd6-4e30-ae71-d56b9fe70924",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d7a13d-9499-497c-9cab-968815355219",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cnn.pth'\n",
    "torch.save(model.state_dict(), PATH)\n",
    "print('Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2ce575-fc04-4570-8995-e677a8621885",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for _ in range(29)]\n",
    "    n_class_samples = [0 for _ in range(29)]\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')\n",
    "\n",
    "    for i in range(29):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d530f29-7bd7-4eb4-b8db-f274460631d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet()\n",
    "model.load_state_dict(torch.load('./cnn.pth'))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9077fdc9-6a90-47e8-bb12-d16b40161415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Saved Model\n",
    "def test(name,label):\n",
    "    path = './vid/'+name+'.jpeg'\n",
    "    imag = preprocess_image(path)\n",
    "    showarray(imag)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    imag = transform(imag)\n",
    "    imag = imag.unsqueeze(0)\n",
    "    imag = imag.to(device)\n",
    "    outputs = model(imag)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    print(f'Actual label: {label}, Predicted label: {datatypes[predicted.item()]}')\n",
    "    # Testing Saved Model\n",
    "    imag = cv2.flip(preprocess_image(path),1)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    imag = transform(imag)\n",
    "    imag = imag.unsqueeze(0)\n",
    "    imag = imag.to(device)\n",
    "    outputs = model(imag)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    print(f'Actual label: {label}, Predicted label: {datatypes[predicted.item()]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b59002-6e58-41a0-ac5e-aa69ac1d5156",
   "metadata": {},
   "outputs": [],
   "source": [
    "test('A','A')\n",
    "test('B','B')\n",
    "test('C','C')\n",
    "test('D','D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d438a990-f717-4903-ad94-1d20b7eecd93",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Legacy Code\n",
    "def setup(video_path):\n",
    "    # Function to display image in Jupyter Notebook\n",
    "    # Start capturing video from MP4 file\n",
    "    return cv2.VideoCapture(video_path)\n",
    "cap = setup('/home/aravos/vid/vid1.mp4')\n",
    "\n",
    "mpHands = mp.solutions.hands\n",
    "hands = mpHands.Hands(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=1,\n",
    "    min_detection_confidence=0.75,\n",
    "    min_tracking_confidence=0.75,\n",
    "    max_num_hands=2\n",
    ")\n",
    "\n",
    "while True:\n",
    "    # Read video frame by frame\n",
    "    success, img = cap.read()\n",
    "\n",
    "    if not success:\n",
    "        print('NO')\n",
    "        break\n",
    "\n",
    "    # Flip the image(frame)\n",
    "    img = cv2.flip(img, 1)\n",
    "\n",
    "    # Convert BGR image to RGB image\n",
    "    imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the RGB image\n",
    "    results = hands.process(imgRGB)\n",
    "\n",
    "    # If hands are present in image(frame)\n",
    "    if results.multi_hand_landmarks:\n",
    "\n",
    "        # Both Hands are present in image(frame)\n",
    "        if len(results.multi_handedness) == 2:\n",
    "            cv2.putText(img, 'Both Hands', (250, 50),\n",
    "                        cv2.FONT_HERSHEY_COMPLEX,\n",
    "                        0.9, (0, 255, 0), 2)\n",
    "\n",
    "        # If any hand present\n",
    "        else:\n",
    "            for i in results.multi_handedness:\n",
    "\n",
    "                # Return whether it is Right or Left Hand\n",
    "                label = MessageToDict(i)['classification'][0]['label']\n",
    "\n",
    "                if label == 'Left':\n",
    "                    cv2.putText(img, label + ' Hand',\n",
    "                                (20, 50),\n",
    "                                cv2.FONT_HERSHEY_COMPLEX,\n",
    "                                0.9, (0, 255, 0), 2)\n",
    "\n",
    "                if label == 'Right':\n",
    "                    cv2.putText(img, label + ' Hand', (460, 50),\n",
    "                                cv2.FONT_HERSHEY_COMPLEX,\n",
    "                                0.9, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the image using showarray function\n",
    "    showarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    # Clear the previous output for the next frame\n",
    "    clear_output(wait=True)\n",
    "cap = setup('/home/aravos/vid/vid1.mp4')\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "    img = cv2.flip(img, 1)\n",
    "    k = 3\n",
    "    t = 2\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    #img_blur = cv2.GaussianBlur(img_gray,(25,25),1.5) - cv2.GaussianBlur(img_gray,(25,25),1.5*k)\n",
    "    img_blur = cv2.GaussianBlur(img_gray,(15,15),1.5) - t * cv2.GaussianBlur(img_gray,(15,15),1.5*k)\n",
    "    _, img_thresh = cv2.threshold(img_blur, 170, 255, cv2.THRESH_BINARY)\n",
    "    # showarray(img_thresh)\n",
    "    # showarray(cv2.Canny(img_blur,100,150))\n",
    "    final_img = cv2.Canny(img,150,200) + img_thresh\n",
    "    showarray(final_img)\n",
    "    # Clear the previous output for the next frame\n",
    "    time.sleep(0.05)\n",
    "    clear_output(wait=True)\n",
    "# Extended Gaussian\n",
    "cap = setup('/home/aravos/vid/vid1.mp4')\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "    img = cv2.flip(img, 1)\n",
    "    k = 3\n",
    "    t = 5\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    #img_blur = cv2.GaussianBlur(img_gray,(25,25),1.5) - cv2.GaussianBlur(img_gray,(25,25),1.5*k)\n",
    "    img_blur = (1+t) * cv2.GaussianBlur(img_gray,(15,15),1.5) - t * cv2.GaussianBlur(img_gray,(15,15),1.5*k)\n",
    "    _, img_thresh = cv2.threshold(img_blur, 170, 255, cv2.THRESH_BINARY)\n",
    "    #showarray(img_thresh)\n",
    "    showarray(cv2.Canny(img_thresh,100,150))\n",
    "    time.sleep(0.05)\n",
    "    clear_output(wait=True)\n",
    "#Sobel + Laplacian\n",
    "cap = setup('/home/aravos/vid/handsvid.mp4')\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "    img = cv2.flip(img, 1)\n",
    "    k = 3\n",
    "    t = 2\n",
    "    # Convert to HSV for simpler calculations \n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV) \n",
    "      \n",
    "    # Calculation of Sobelx \n",
    "    sobelx = cv2.Sobel(frame,cv2.CV_64F,1,0,ksize=5) \n",
    "      \n",
    "    # Calculation of Sobely \n",
    "    sobely = cv2.Sobel(frame,cv2.CV_64F,0,1,ksize=5) \n",
    "      \n",
    "    # Calculation of Laplacian \n",
    "    laplacian = cv2.Laplacian(frame,cv2.CV_64F) \n",
    "    \n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img_blur = cv2.GaussianBlur(img_gray,(9,9),0) \n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV) \n",
    "      \n",
    "    # Calculation of Sobelx \n",
    "    sobelx = cv2.Sobel(frame,cv2.CV_64F,1,0,ksize=5) \n",
    "      \n",
    "    # Calculation of Sobely \n",
    "    sobely = cv2.Sobel(frame,cv2.CV_64F,0,1,ksize=5) \n",
    "      \n",
    "    # Calculation of Laplacian \n",
    "    laplacian = cv2.Laplacian(frame,cv2.CV_64F) \n",
    "    _, img_thresh = cv2.threshold(img_blur, 170, 255, cv2.THRESH_BINARY)\n",
    "    # showarray(img_thresh)\n",
    "    # showarray(cv2.Canny(img_blur,100,150))\n",
    "    final_img = cv2.Canny(img,150,200) + img_thresh\n",
    "    showarray(final_img)\n",
    "    # Clear the previous output for the next frame\n",
    "    time.sleep(0.05)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "cap = setup('/home/aravos/vid/vid3.mp4')\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "    showarray(preprocess_image(img))\n",
    "    time.sleep(0.05)\n",
    "    clear_output(wait=True)\n",
    "    continue\n",
    "    img = cv2.flip(img, 1)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "    # Define range of skin color in HSV\n",
    "    lower_skin = np.array([0, 20, 70])\n",
    "    upper_skin = np.array([25, 255, 255])\n",
    "\n",
    "    # Create a mask for skin color detection\n",
    "    mask_skin = cv2.inRange(img_hsv, lower_skin, upper_skin)\n",
    "\n",
    "    # Apply Gaussian blur to the image\n",
    "    img_blur = cv2.GaussianBlur(img, (3, 3), 0)\n",
    "\n",
    "    # Convert image to grayscale\n",
    "    img_gray = cv2.cvtColor(img_blur, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Sobel Edge Detection on skin color-masked grayscale image\n",
    "    sobelx = cv2.Sobel(img_gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    sobely = cv2.Sobel(img_gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    laplacian = cv2.Laplacian(img_gray,cv2.CV_64F) \n",
    "    sobel = np.sqrt(sobelx**2 + sobely**2)\n",
    "\n",
    "    # Apply the skin mask to the Sobel edge image\n",
    "    img = cv2.bitwise_and(sobel, sobel, mask=mask_skin)\n",
    "    img = np.uint8(np.clip(img, 0, 255))\n",
    "    k = 10\n",
    "    a = 5\n",
    "    s = 7\n",
    "    #img = cv2.GaussianBlur(img, (9, 9),s)\n",
    "    img = (1+k) * cv2.GaussianBlur(img, (9, 9),s)  - k * cv2.GaussianBlur(img, (9, 9),s*a)\n",
    "    img = cv2.bilateralFilter(img,9,75,75)\n",
    "    img = cv2.GaussianBlur(img, (9, 9),s)\n",
    "    _, img = cv2.threshold(img, 25, 50, cv2.THRESH_BINARY)\n",
    "    \n",
    "    #img = cv2.Canny(img,25,150)\n",
    "    \n",
    "    showarray(img)\n",
    "    time.sleep(0.05)\n",
    "    clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4ce012-156c-4b1b-9296-bcad39d90794",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "classes = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space')\n",
    "    \n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "l = []\n",
    "for i in range (len(images)):\n",
    "    img = images[i].numpy()\n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    img = np.uint8(img * 255)\n",
    "    l.append(img)\n",
    "# show images\n",
    "showarray(l)\n",
    "\n",
    "conv1 = nn.Conv2d(3, 6, 5) # _ = input channel size (3 bcuz RGB), _ = output channel size, _= kernel size i.e size of the smaller matrix used in for convolution in cnn\n",
    "pool = nn.MaxPool2d(2, 2) # _= kernel size (2x2) its pooled hence it's smaller, _ = stride i.e what it's shifted by 2\n",
    "conv2 = nn.Conv2d(6, 16, 5) # Input channel size must be EQUAL to the last output\n",
    "conv3 = nn.Conv2d(16, 40, 5)\n",
    "conv4 = nn.Conv2d(40, 100, 5)\n",
    "conv5 = nn.Conv2d(16, 40, 5)\n",
    "conv6 = nn.Conv2d(16, 40, 5)\n",
    "\n",
    "print(images.shape)\n",
    "x = conv1(images)\n",
    "print(x.shape)\n",
    "print(F.relu(x).shape)\n",
    "x = conv2(x)\n",
    "print(x.shape)\n",
    "x = pool(x)\n",
    "print(x.shape)\n",
    "x = conv3(x)\n",
    "print(x.shape)\n",
    "x = conv4(x)\n",
    "print(x.shape)\n",
    "x = pool(x)\n",
    "print(x.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
